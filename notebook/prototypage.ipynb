{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2361280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SYST√àME RAG OPTIMIS√â avec CHUNKING INTELLIGENT\n",
      "Mod√®le: gemma2:2b\n",
      "======================================================================\n",
      "\n",
      "üîç V√©rification d'Ollama...\n",
      "‚úÖ Ollama OK\n",
      "\n",
      "üìù Cr√©ation des chunks intelligents...\n",
      "Encodage d√©tect√©: latin-1\n",
      "Nombre de chunks cr√©√©s: 19\n",
      "Premier chunk: h: U B S bonjour c: oui bonjour e j'appelle je sais pas si j'appelle au bon endroit e...\n",
      "\n",
      "üóÑÔ∏è  Connexion PostgreSQL...\n",
      "\n",
      "üîß Pr√©paration base de donn√©es...\n",
      "\n",
      "ü§ñ G√©n√©ration embeddings (19 chunks)...\n",
      "   [1/19] ‚úì\n",
      "   [2/19] ‚úì\n",
      "   [3/19] ‚úì\n",
      "   [4/19] ‚úì\n",
      "   [5/19] ‚úì\n",
      "   [6/19] ‚úì\n",
      "   [7/19] ‚úì\n",
      "   [8/19] ‚úì\n",
      "   [9/19] ‚úì\n",
      "   [10/19] ‚úì\n",
      "   [11/19] ‚úì\n",
      "   [12/19] ‚úì\n",
      "   [13/19] ‚úì\n",
      "   [14/19] ‚úì\n",
      "   [15/19] ‚úì\n",
      "   [16/19] ‚úì\n",
      "   [17/19] ‚úì\n",
      "   [18/19] ‚úì\n",
      "   [19/19] ‚úì\n",
      "\n",
      "‚úÖ Embeddings: 19 OK, 0 KO\n",
      "\n",
      "======================================================================\n",
      "üí¨ MODE QUESTIONS-R√âPONSES\n",
      "======================================================================\n",
      "Tapez 'quit' pour sortir\n",
      "\n",
      "\n",
      "üîç Recherche contexte pertinent (top-5)...\n",
      "\n",
      "üìÑ Contexte s√©lectionn√©:\n",
      "\n",
      "  [1] (similarit√©: 80.3%)\n",
      "      h: ben √©coutez vous avez c: son espagnol h: un organisme qui s'appelle e \"English Connection\" il me semble...\n",
      "\n",
      "  [2] (similarit√©: 76.1%)\n",
      "      h: je vous √©coute c: c'est pour c: e c'est pour savoir si la fac pendant l'√©t√© e a des professeurs ou des des gens qui f...\n",
      "\n",
      "  [3] (similarit√©: 74.7%)\n",
      "      h: oui alors e la fac de e de lettre et de langues se trouve √† Lorient donc il faudrait plut√¥t  voir avec Lorient pour e...\n",
      "\n",
      "  [4] (similarit√©: 68.1%)\n",
      "      c: ouais et e bon elle va passer en premi√®re mais ils faut ils faut ils faut qu'elle renforce s√©rieusement son anglais e...\n",
      "\n",
      "  [5] (similarit√©: 67.7%)\n",
      "      c: √† d'accord je vous remercie h: oui c: madame h: je vous en prie...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ü§ñ G√©n√©ration r√©ponse...\n",
      "\n",
      "   Voici les informations que nous pouvons d√©duire du contexte : \n",
      "\n",
      "* **English Connection** est un organisme qui propose des stages d'anglais et espagnol. \n",
      "* La fac de lettres et de langues √† Lorient pourrait organiser des stages, mais il faut contacter directement la fac pour en savoir plus. \n",
      "* La fac est ferm√©e du 23 juillet au 23 ao√ªt. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üí° R√âPONSE:\n",
      "======================================================================\n",
      "Voici les informations que nous pouvons d√©duire du contexte : \n",
      "\n",
      "* **English Connection** est un organisme qui propose des stages d'anglais et espagnol. \n",
      "* La fac de lettres et de langues √† Lorient pourrait organiser des stages, mais il faut contacter directement la fac pour en savoir plus. \n",
      "* La fac est ferm√©e du 23 juillet au 23 ao√ªt.\n",
      "======================================================================\n",
      "\n",
      "\n",
      "üîç Recherche contexte pertinent (top-5)...\n",
      "\n",
      "üìÑ Contexte s√©lectionn√©:\n",
      "\n",
      "  [1] (similarit√©: 80.3%)\n",
      "      h: oui alors e la fac de e de lettre et de langues se trouve √† Lorient donc il faudrait plut√¥t  voir avec Lorient pour e...\n",
      "\n",
      "  [2] (similarit√©: 73.7%)\n",
      "      h: ben √©coutez vous avez c: son espagnol h: un organisme qui s'appelle e \"English Connection\" il me semble...\n",
      "\n",
      "  [3] (similarit√©: 72.6%)\n",
      "      h: je vous √©coute c: c'est pour c: e c'est pour savoir si la fac pendant l'√©t√© e a des professeurs ou des des gens qui f...\n",
      "\n",
      "  [4] (similarit√©: 70.2%)\n",
      "      c: ouais et e bon elle va passer en premi√®re mais ils faut ils faut ils faut qu'elle renforce s√©rieusement son anglais e...\n",
      "\n",
      "  [5] (similarit√©: 69.8%)\n",
      "      c: √† d'accord je vous remercie h: oui c: madame h: je vous en prie...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "ü§ñ G√©n√©ration r√©ponse...\n",
      "\n",
      "   Pour des stages de langues, vous devez contacter : \n",
      "\n",
      "* **La facult√© de Lorient** (en fonction de l'information que vous avez re√ßue) \n",
      "* **English Connection** (un organisme qui propose des stages d'anglais et d'espagnol) \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "======================================================================\n",
      "üí° R√âPONSE:\n",
      "======================================================================\n",
      "Pour des stages de langues, vous devez contacter : \n",
      "\n",
      "* **La facult√© de Lorient** (en fonction de l'information que vous avez re√ßue) \n",
      "* **English Connection** (un organisme qui propose des stages d'anglais et d'espagnol)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "üëã Au revoir!\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "conversation_file_path = \"C:\\\\Users\\\\lenovo\\\\Chatbot-RAG\\\\data\\\\TRANS_TXT\\\\017_00000012.txt\"\n",
    "DB_CONNECTION_STR = \"dbname=postgres user=postgres password=zaineb host=localhost port=5434\"\n",
    "\n",
    "# OLLAMA\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "OLLAMA_EMBED_MODEL = \"nomic-embed-text\"\n",
    "LLM_MODEL = \"gemma2:2b\"\n",
    "\n",
    "VECTOR_DIM = 4096\n",
    "EMBED_TIMEOUT = 60\n",
    "GENERATE_TIMEOUT = 90\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# -----------------------\n",
    "# Chunking intelligent\n",
    "# -----------------------\n",
    "def create_conversation_chunks(file_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Cr√©e des chunks intelligents en regroupant les r√©pliques courtes\n",
    "    et en gardant le contexte conversationnel\n",
    "    \"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "    encoding = 'latin-1'\n",
    "    \n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=enc) as f:\n",
    "                f.read()\n",
    "            encoding = enc\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Encodage d√©tect√©: {encoding}\")\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=encoding) as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_text = \"\"\n",
    "    MIN_CHUNK_LENGTH = 50  # Minimum de caract√®res par chunk\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Ignorer les lignes vides et les tags\n",
    "        if not line or line.startswith(\"<\"):\n",
    "            # Si on a un chunk en cours et qu'on rencontre un tag, on le sauvegarde\n",
    "            if current_chunk and len(current_text) >= MIN_CHUNK_LENGTH:\n",
    "                chunks.append({\n",
    "                    'text': current_text.strip(),\n",
    "                    'lines': current_chunk.copy()\n",
    "                })\n",
    "                current_chunk = []\n",
    "                current_text = \"\"\n",
    "            continue\n",
    "        \n",
    "        # Nettoyer la ligne\n",
    "        cleaned = line.removeprefix(\"     \").strip()\n",
    "        \n",
    "        # Ajouter √† la chunk courante\n",
    "        current_chunk.append(cleaned)\n",
    "        current_text += \" \" + cleaned\n",
    "        \n",
    "        # Si le chunk est assez long ET se termine par une phrase compl√®te, on le sauvegarde\n",
    "        if len(current_text) >= MIN_CHUNK_LENGTH:\n",
    "            # V√©rifier si c'est une fin de phrase significative\n",
    "            if any(current_text.endswith(marker) for marker in ['.', '?', '!']) or len(current_chunk) >= 3:\n",
    "                chunks.append({\n",
    "                    'text': current_text.strip(),\n",
    "                    'lines': current_chunk.copy()\n",
    "                })\n",
    "                current_chunk = []\n",
    "                current_text = \"\"\n",
    "    \n",
    "    # Sauvegarder le dernier chunk s'il existe\n",
    "    if current_chunk and len(current_text) >= MIN_CHUNK_LENGTH:\n",
    "        chunks.append({\n",
    "            'text': current_text.strip(),\n",
    "            'lines': current_chunk.copy()\n",
    "        })\n",
    "    \n",
    "    print(f\"Nombre de chunks cr√©√©s: {len(chunks)}\")\n",
    "    print(f\"Premier chunk: {chunks[0]['text'][:100]}...\" if chunks else \"Aucun chunk\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# -----------------------\n",
    "# Fonctions utilitaires\n",
    "# -----------------------\n",
    "def check_ollama_health() -> bool:\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def calculate_embeddings_ollama(text: str, retry_count: int = 3) -> list[float]:\n",
    "    payload = {\"model\": OLLAMA_EMBED_MODEL, \"prompt\": text}\n",
    "    \n",
    "    for attempt in range(retry_count):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{OLLAMA_URL}/api/embeddings\", \n",
    "                json=payload, \n",
    "                timeout=EMBED_TIMEOUT\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            embedding = response.json().get(\"embedding\", [])\n",
    "            if embedding:\n",
    "                return embedding\n",
    "        except requests.exceptions.Timeout:\n",
    "            if attempt < retry_count - 1:\n",
    "                time.sleep(2)\n",
    "        except Exception as e:\n",
    "            if attempt < retry_count - 1:\n",
    "                time.sleep(2)\n",
    "    \n",
    "    return []\n",
    "\n",
    "def embedding_to_pgvector_format(emb: list[float]) -> str:\n",
    "    return \"[\" + \",\".join(map(str, emb)) + \"]\"\n",
    "\n",
    "def save_embedding(corpus: str, embedding: list[float], cursor) -> None:\n",
    "    emb_literal = embedding_to_pgvector_format(embedding)\n",
    "    cursor.execute(\n",
    "        \"\"\"INSERT INTO embeddings (corpus, embedding) VALUES (%s, %s::vector)\"\"\",\n",
    "        (corpus, emb_literal)\n",
    "    )\n",
    "\n",
    "def similar_corpus(input_corpus: str, cursor, top_k: int = 5) -> list[tuple]:\n",
    "    \"\"\"Recherche avec distance cosine\"\"\"\n",
    "    emb = calculate_embeddings_ollama(input_corpus)\n",
    "    if not emb:\n",
    "        return []\n",
    "    emb_literal = embedding_to_pgvector_format(emb)\n",
    "    \n",
    "    cursor.execute(\n",
    "        f\"\"\"\n",
    "        SELECT id, corpus, embedding <=> %s::vector AS distance\n",
    "        FROM embeddings\n",
    "        ORDER BY embedding <=> %s::vector\n",
    "        LIMIT {top_k}\n",
    "        \"\"\",\n",
    "        (emb_literal, emb_literal)\n",
    "    )\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def generate_answer_ollama(user_query: str, context_texts: list[str]) -> str:\n",
    "    if not context_texts:\n",
    "        return \"Aucun contexte trouv√© pour r√©pondre √† la question.\"\n",
    "    \n",
    "    # Combiner tous les contextes\n",
    "    full_context = \"\\n\\n\".join([f\"Extrait {i+1}:\\n{text}\" for i, text in enumerate(context_texts)])\n",
    "    \n",
    "    prompt = f\"\"\"Tu es un assistant qui r√©pond en utilisant UNIQUEMENT le contexte fourni.\n",
    "\n",
    "CONTEXTE (conversation service client):\n",
    "{full_context}\n",
    "\n",
    "QUESTION: {user_query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Lis TOUT le contexte attentivement\n",
    "- Liste TOUS les organismes, lieux, dates et d√©tails mentionn√©s\n",
    "- Structure ta r√©ponse clairement avec des puces si n√©cessaire\n",
    "- N'invente RIEN qui n'est pas explicitement dans le contexte\n",
    "- Si une information manque, dis-le clairement\n",
    "\n",
    "R√âPONSE COMPL√àTE:\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.05,\n",
    "        \"stream\": True,\n",
    "        \"options\": {\n",
    "            \"num_predict\": 300,\n",
    "            \"top_k\": 5,\n",
    "            \"top_p\": 0.3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/api/generate\", \n",
    "            json=payload, \n",
    "            stream=True,\n",
    "            timeout=GENERATE_TIMEOUT\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        full_response = \"\"\n",
    "        print(\"   \", end=\"\", flush=True)\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                try:\n",
    "                    json_response = json.loads(line)\n",
    "                    chunk = json_response.get(\"response\", \"\")\n",
    "                    full_response += chunk\n",
    "                    print(chunk, end=\"\", flush=True)\n",
    "                    if json_response.get(\"done\", False):\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        \n",
    "        print()\n",
    "        return full_response.strip() if full_response else \"Erreur : r√©ponse vide\"\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"‚è±Ô∏è Timeout\"\n",
    "    except Exception as e:\n",
    "        return f\"Erreur : {e}\"\n",
    "\n",
    "# -----------------------\n",
    "# Pipeline principal\n",
    "# -----------------------\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SYST√àME RAG OPTIMIS√â avec CHUNKING INTELLIGENT\")\n",
    "    print(f\"Mod√®le: {LLM_MODEL}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\nüîç V√©rification d'Ollama...\")\n",
    "    if not check_ollama_health():\n",
    "        print(\"‚ùå Ollama non accessible\")\n",
    "        return\n",
    "    print(\"‚úÖ Ollama OK\")\n",
    "    \n",
    "    print(\"\\nüìù Cr√©ation des chunks intelligents...\")\n",
    "    chunks = create_conversation_chunks(conversation_file_path)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"‚ùå Aucun chunk cr√©√©!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüóÑÔ∏è  Connexion PostgreSQL...\")\n",
    "    try:\n",
    "        with psycopg.connect(DB_CONNECTION_STR) as conn:\n",
    "            conn.autocommit = True\n",
    "            with conn.cursor() as cur:\n",
    "                print(\"\\nüîß Pr√©paration base de donn√©es...\")\n",
    "                cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "                cur.execute(f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS embeddings (\n",
    "                        id SERIAL PRIMARY KEY,\n",
    "                        corpus TEXT,\n",
    "                        embedding vector({VECTOR_DIM})\n",
    "                    )\n",
    "                \"\"\")\n",
    "                cur.execute(\"DELETE FROM embeddings\")\n",
    "                \n",
    "                print(f\"\\nü§ñ G√©n√©ration embeddings ({len(chunks)} chunks)...\")\n",
    "                \n",
    "                successful = 0\n",
    "                failed = 0\n",
    "                \n",
    "                for i, chunk in enumerate(chunks, 1):\n",
    "                    print(f\"   [{i}/{len(chunks)}] \", end=\"\", flush=True)\n",
    "                    emb = calculate_embeddings_ollama(chunk['text'])\n",
    "                    if emb:\n",
    "                        save_embedding(chunk['text'], emb, cur)\n",
    "                        successful += 1\n",
    "                        print(\"‚úì\")\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                        print(\"‚úó\")\n",
    "                    \n",
    "                    if i % BATCH_SIZE == 0:\n",
    "                        time.sleep(0.5)\n",
    "                \n",
    "                conn.commit()\n",
    "                print(f\"\\n‚úÖ Embeddings: {successful} OK, {failed} KO\")\n",
    "                \n",
    "                if successful == 0:\n",
    "                    print(\"‚ùå √âchec cr√©ation embeddings\")\n",
    "                    return\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(\"üí¨ MODE QUESTIONS-R√âPONSES\")\n",
    "                print(\"=\"*70)\n",
    "                print(\"Tapez 'quit' pour sortir\\n\")\n",
    "                \n",
    "                while True:\n",
    "                    user_query = input(\"‚ùì Votre question : \").strip()\n",
    "                    if user_query.lower() in ['quit', 'exit', 'q']:\n",
    "                        print(\"\\nüëã Au revoir!\")\n",
    "                        break\n",
    "                    if not user_query:\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"\\nüîç Recherche contexte pertinent (top-5)...\")\n",
    "                    results = similar_corpus(user_query, cur, top_k=5)\n",
    "                    \n",
    "                    if results:\n",
    "                        context_texts = [r[1] for r in results]\n",
    "                        \n",
    "                        print(\"\\nüìÑ Contexte s√©lectionn√©:\")\n",
    "                        for i, (_, corpus, distance) in enumerate(results):\n",
    "                            similarity = max(0, 1 - distance) * 100\n",
    "                            print(f\"\\n  [{i+1}] (similarit√©: {similarity:.1f}%)\")\n",
    "                            print(f\"      {corpus[:120]}...\")\n",
    "                        \n",
    "                        print(\"\\n\" + \"-\"*70)\n",
    "                        print(\"ü§ñ G√©n√©ration r√©ponse...\\n\")\n",
    "                        answer = generate_answer_ollama(user_query, context_texts)\n",
    "                        \n",
    "                        print(\"\\n\" + \"=\"*70)\n",
    "                        print(\"üí° R√âPONSE:\")\n",
    "                        print(\"=\"*70)\n",
    "                        print(answer)\n",
    "                        print(\"=\"*70 + \"\\n\")\n",
    "                    else:\n",
    "                        print(\"‚ùå Aucun contexte trouv√©\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
