{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2361280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYST√àME RAG avec OLLAMA (gemma2:2b)\n",
      "============================================================\n",
      "\n",
      "üîç V√©rification d'Ollama...\n",
      "‚úÖ Ollama est accessible\n",
      "Encodage d√©tect√©: latin-1\n",
      "Nombre de phrases: 43\n",
      "Premi√®res phrases: ['h: U B S bonjour', \"c: oui bonjour e j'appelle je sais pas si j'appelle au bon endroit e\", 'h: je vous √©coute']\n",
      "\n",
      "üóÑÔ∏è  Connexion √† PostgreSQL...\n",
      "\n",
      "üîß Pr√©paration de la base...\n",
      "\n",
      "ü§ñ G√©n√©ration des embeddings (43 phrases)...\n",
      "   Patience...\n",
      "   [1/43] ‚úì\n",
      "   [2/43] ‚úì\n",
      "   [3/43] ‚úì\n",
      "   [4/43] ‚úì\n",
      "   [5/43] ‚úì\n",
      "   [6/43] ‚úì\n",
      "   [7/43] ‚úì\n",
      "   [8/43] ‚úì\n",
      "   [9/43] ‚úì\n",
      "   [10/43] ‚úì\n",
      "   [11/43] ‚úì\n",
      "   [12/43] ‚úì\n",
      "   [13/43] ‚úì\n",
      "   [14/43] ‚úì\n",
      "   [15/43] ‚úì\n",
      "   [16/43] ‚úì\n",
      "   [17/43] ‚úì\n",
      "   [18/43] ‚úì\n",
      "   [19/43] ‚úì\n",
      "   [20/43] ‚úì\n",
      "   [21/43] ‚úì\n",
      "   [22/43] ‚úì\n",
      "   [23/43] ‚úì\n",
      "   [24/43] ‚úì\n",
      "   [25/43] ‚úì\n",
      "   [26/43] ‚úì\n",
      "   [27/43] ‚úì\n",
      "   [28/43] ‚úì\n",
      "   [29/43] ‚úì\n",
      "   [30/43] ‚úì\n",
      "   [31/43] ‚úì\n",
      "   [32/43] ‚úì\n",
      "   [33/43] ‚úì\n",
      "   [34/43] ‚úì\n",
      "   [35/43] ‚úì\n",
      "   [36/43] ‚úì\n",
      "   [37/43] ‚úì\n",
      "   [38/43] ‚úì\n",
      "   [39/43] ‚úì\n",
      "   [40/43] ‚úì\n",
      "   [41/43] ‚úì\n",
      "   [42/43] ‚úì\n",
      "   [43/43] ‚úì\n",
      "\n",
      "‚úÖ Embeddings: 43 r√©ussis, 0 √©chou√©s\n",
      "\n",
      "============================================================\n",
      "üí¨ MODE INTERACTIF\n",
      "============================================================\n",
      "Tapez 'quit' pour sortir\n",
      "\n",
      "\n",
      "üîç Recherche (top-10)...\n",
      "\n",
      "üìÑ Contexte trouv√© :\n",
      "  [1] h: oui alors e la fac de e de lettre et de langues se trouve √† Lorient... (dist: 0.197)\n",
      "  [2] c: e c'est pour savoir si la fac pendant l'√©t√© e a des professeurs ou ... (dist: 0.294)\n",
      "  [3] c: ouais et e bon elle va passer en premi√®re mais ils faut ils faut il... (dist: 0.298)\n",
      "  [4] c: √† d'accord je vous remercie... (dist: 0.307)\n",
      "  [5] h: c'est place de la r√©publique c'est c'est tout pr√®s de la poste... (dist: 0.308)\n",
      "  [6] h: e non √† priori non je vois pas e --- e(lle) elle est e vous d√Ætes e... (dist: 0.325)\n",
      "  [7] c: son espagnol... (dist: 0.365)\n",
      "  [8] c: en seconde et... (dist: 0.395)\n",
      "  [9] c: c'est place de... (dist: 0.396)\n",
      "  [10] c: bonne journ√©e... (dist: 0.408)\n",
      "\n",
      "ü§ñ G√©n√©ration de la r√©ponse...\n",
      "\n",
      "   Pour des stages de langues, il faut contacter la facult√© de Lorient. \n",
      "\n",
      "\n",
      "============================================================\n",
      "üí° R√âPONSE:\n",
      "Pour des stages de langues, il faut contacter la facult√© de Lorient.\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîç Recherche (top-10)...\n",
      "\n",
      "üìÑ Contexte trouv√© :\n",
      "  [1] h: e du cot√© de la place de la r√©publique l√† e sinon e je sais que la ... (dist: 0.299)\n",
      "  [2] h: e ce serait pour vous vous souhaiteriez... (dist: 0.337)\n",
      "  [3] h: et elle souhaiterais se perfectionner... (dist: 0.357)\n",
      "\n",
      "ü§ñ G√©n√©ration de la r√©ponse...\n",
      "\n",
      "   Le contexte ne mentionne aucune information sur la fermeture de la facult√© pendant l'√©t√©. \n",
      " \n",
      "**Organismes, lieux et informations mentionn√©s:**\n",
      "* H√¥tesse (h)\n",
      "* Client (c)\n",
      "* Place de la R√©publique\n",
      "* Chambre de commerce \n",
      "\n",
      "\n",
      "**Informations manquantes:**\n",
      "* Dates d'ouverture et de fermeture de la facult√© \n",
      "\n",
      "\n",
      "============================================================\n",
      "üí° R√âPONSE:\n",
      "Le contexte ne mentionne aucune information sur la fermeture de la facult√© pendant l'√©t√©. \n",
      " \n",
      "**Organismes, lieux et informations mentionn√©s:**\n",
      "* H√¥tesse (h)\n",
      "* Client (c)\n",
      "* Place de la R√©publique\n",
      "* Chambre de commerce \n",
      "\n",
      "\n",
      "**Informations manquantes:**\n",
      "* Dates d'ouverture et de fermeture de la facult√©\n",
      "============================================================\n",
      "\n",
      "üëã Au revoir!\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "conversation_file_path = \"C:\\\\Users\\\\lenovo\\\\Chatbot-RAG\\\\data\\\\TRANS_TXT\\\\017_00000012.txt\"\n",
    "DB_CONNECTION_STR = \"dbname=postgres user=postgres password=zaineb host=localhost port=5434\"\n",
    "\n",
    "# OLLAMA\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "OLLAMA_EMBED_MODEL = \"nomic-embed-text\"\n",
    "LLM_MODEL = \"gemma2:2b\"\n",
    "\n",
    "VECTOR_DIM = 4096\n",
    "EMBED_TIMEOUT = 60\n",
    "GENERATE_TIMEOUT = 90\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# -----------------------\n",
    "# Fonctions utilitaires\n",
    "# -----------------------\n",
    "def detect_encoding(file_path: str) -> str:\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                file.read()\n",
    "            return encoding\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return 'latin-1'\n",
    "\n",
    "def create_conversation_list(file_path: str) -> list[str]:\n",
    "    encoding = detect_encoding(file_path)\n",
    "    print(f\"Encodage d√©tect√©: {encoding}\")\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=encoding) as file:\n",
    "        text = file.read()\n",
    "        text_list = text.split(\"\\n\")\n",
    "        filtered_list = [\n",
    "            chaine.removeprefix(\"     \")\n",
    "            for chaine in text_list\n",
    "            if chaine.strip() and not chaine.startswith(\"<\")\n",
    "        ]\n",
    "        print(f\"Nombre de phrases: {len(filtered_list)}\")\n",
    "        print(\"Premi√®res phrases:\", filtered_list[:3])\n",
    "        return filtered_list\n",
    "\n",
    "def check_ollama_health() -> bool:\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def calculate_embeddings_ollama(text: str, retry_count: int = 3) -> list[float]:\n",
    "    payload = {\"model\": OLLAMA_EMBED_MODEL, \"prompt\": text}\n",
    "    \n",
    "    for attempt in range(retry_count):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{OLLAMA_URL}/api/embeddings\", \n",
    "                json=payload, \n",
    "                timeout=EMBED_TIMEOUT\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            embedding = response.json().get(\"embedding\", [])\n",
    "            if embedding:\n",
    "                return embedding\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"  ‚è±Ô∏è Timeout tentative {attempt + 1}/{retry_count} pour: {text[:50]}...\")\n",
    "            if attempt < retry_count - 1:\n",
    "                time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Erreur tentative {attempt + 1}/{retry_count}: {e}\")\n",
    "            if attempt < retry_count - 1:\n",
    "                time.sleep(2)\n",
    "    \n",
    "    print(f\"  ‚ö†Ô∏è √âchec apr√®s {retry_count} tentatives pour: {text[:50]}...\")\n",
    "    return []\n",
    "\n",
    "def embedding_to_pgvector_format(emb: list[float]) -> str:\n",
    "    return \"[\" + \",\".join(map(str, emb)) + \"]\"\n",
    "\n",
    "def save_embedding(corpus: str, embedding: list[float], cursor) -> None:\n",
    "    emb_literal = embedding_to_pgvector_format(embedding)\n",
    "    cursor.execute(\n",
    "        \"\"\"INSERT INTO embeddings (corpus, embedding) VALUES (%s, %s::vector)\"\"\",\n",
    "        (corpus, emb_literal)\n",
    "    )\n",
    "\n",
    "def similar_corpus(input_corpus: str, cursor, top_k: int = 10) -> list[tuple]:\n",
    "    \"\"\"Recherche avec distance cosine - TOP_K AUGMENT√â √Ä 10\"\"\"\n",
    "    emb = calculate_embeddings_ollama(input_corpus)\n",
    "    if not emb:\n",
    "        return []\n",
    "    emb_literal = embedding_to_pgvector_format(emb)\n",
    "    \n",
    "    # Distance cosine (<=>)\n",
    "    cursor.execute(\n",
    "        f\"\"\"\n",
    "        SELECT id, corpus, embedding <=> %s::vector AS distance\n",
    "        FROM embeddings\n",
    "        ORDER BY embedding <=> %s::vector\n",
    "        LIMIT {top_k}\n",
    "        \"\"\",\n",
    "        (emb_literal, emb_literal)\n",
    "    )\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def generate_answer_ollama(user_query: str, context_texts: list[str]) -> str:\n",
    "    if not context_texts:\n",
    "        return \"Aucun contexte trouv√© pour r√©pondre √† la question.\"\n",
    "    \n",
    "    # Num√©roter clairement chaque √©l√©ment de contexte\n",
    "    context = \"\\n\".join([f\"[{i+1}] {text}\" for i, text in enumerate(context_texts)])\n",
    "    \n",
    "    # Prompt d√©taill√© avec instructions strictes\n",
    "    prompt = f\"\"\"Tu es un assistant qui r√©pond en utilisant UNIQUEMENT les informations du contexte.\n",
    "\n",
    "CONTEXTE (conversation entre hotesse 'h:' et client 'c:'):\n",
    "{context}\n",
    "\n",
    "QUESTION: {user_query}\n",
    "\n",
    "INSTRUCTIONS IMPORTANTES:\n",
    "1. Lis TOUT le contexte attentivement ligne par ligne\n",
    "2. Liste TOUS les organismes, lieux et informations mentionn√©s\n",
    "3. N'oublie AUCUN d√©tail (noms d'organismes, adresses, dates)\n",
    "4. Structure ta r√©ponse clairement\n",
    "5. N'invente RIEN qui n'est pas dans le contexte\n",
    "\n",
    "R√âPONSE COMPL√àTE ET D√âTAILL√âE:\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.05,  # Tr√®s d√©terministe\n",
    "        \"stream\": True,\n",
    "        \"options\": {\n",
    "            \"num_predict\": 250,  # Plus de tokens pour r√©ponse compl√®te\n",
    "            \"top_k\": 5,\n",
    "            \"top_p\": 0.3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/api/generate\", \n",
    "            json=payload, \n",
    "            stream=True,\n",
    "            timeout=GENERATE_TIMEOUT\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        full_response = \"\"\n",
    "        print(\"   \", end=\"\", flush=True)\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                try:\n",
    "                    json_response = json.loads(line)\n",
    "                    chunk = json_response.get(\"response\", \"\")\n",
    "                    full_response += chunk\n",
    "                    print(chunk, end=\"\", flush=True)\n",
    "                    if json_response.get(\"done\", False):\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        \n",
    "        print()\n",
    "        return full_response.strip() if full_response else \"Erreur : r√©ponse vide\"\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"‚è±Ô∏è Timeout : essayez une question plus simple.\"\n",
    "    except Exception as e:\n",
    "        return f\"Erreur : {e}\"\n",
    "\n",
    "# -----------------------\n",
    "# Pipeline principal\n",
    "# -----------------------\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"SYST√àME RAG avec OLLAMA ({LLM_MODEL})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nüîç V√©rification d'Ollama...\")\n",
    "    if not check_ollama_health():\n",
    "        print(\"‚ùå Ollama n'est pas accessible\")\n",
    "        print(\"   D√©marrez-le avec: ollama serve\")\n",
    "        return\n",
    "    print(\"‚úÖ Ollama est accessible\")\n",
    "    \n",
    "    corpus_list = create_conversation_list(conversation_file_path)\n",
    "    if not corpus_list:\n",
    "        print(\"‚ùå Aucun texte trouv√©!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüóÑÔ∏è  Connexion √† PostgreSQL...\")\n",
    "    try:\n",
    "        with psycopg.connect(DB_CONNECTION_STR) as conn:\n",
    "            conn.autocommit = True\n",
    "            with conn.cursor() as cur:\n",
    "                print(\"\\nüîß Pr√©paration de la base...\")\n",
    "                cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "                cur.execute(f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS embeddings (\n",
    "                        id SERIAL PRIMARY KEY,\n",
    "                        corpus TEXT,\n",
    "                        embedding vector({VECTOR_DIM})\n",
    "                    )\n",
    "                \"\"\")\n",
    "                cur.execute(\"DELETE FROM embeddings\")\n",
    "                \n",
    "                print(f\"\\nü§ñ G√©n√©ration des embeddings ({len(corpus_list)} phrases)...\")\n",
    "                print(\"   Patience...\")\n",
    "                \n",
    "                successful = 0\n",
    "                failed = 0\n",
    "                \n",
    "                for i, corpus in enumerate(corpus_list, 1):\n",
    "                    if corpus.strip():\n",
    "                        print(f\"   [{i}/{len(corpus_list)}] \", end=\"\", flush=True)\n",
    "                        emb = calculate_embeddings_ollama(corpus)\n",
    "                        if emb:\n",
    "                            save_embedding(corpus, emb, cur)\n",
    "                            successful += 1\n",
    "                            print(\"‚úì\")\n",
    "                        else:\n",
    "                            failed += 1\n",
    "                            print(\"‚úó\")\n",
    "                        \n",
    "                        if i % BATCH_SIZE == 0:\n",
    "                            time.sleep(0.5)\n",
    "                \n",
    "                conn.commit()\n",
    "                print(f\"\\n‚úÖ Embeddings: {successful} r√©ussis, {failed} √©chou√©s\")\n",
    "                \n",
    "                if successful == 0:\n",
    "                    print(\"‚ùå Impossible de continuer sans embeddings\")\n",
    "                    return\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"üí¨ MODE INTERACTIF\")\n",
    "                print(\"=\"*60)\n",
    "                print(\"Tapez 'quit' pour sortir\\n\")\n",
    "                \n",
    "                while True:\n",
    "                    user_query = input(\"‚ùì Question : \").strip()\n",
    "                    if user_query.lower() in ['quit', 'exit', 'q']:\n",
    "                        print(\"üëã Au revoir!\")\n",
    "                        break\n",
    "                    if not user_query:\n",
    "                        continue\n",
    "                    \n",
    "                    print(\"\\nüîç Recherche (top-10)...\")\n",
    "                    results = similar_corpus(user_query, cur, top_k=10)\n",
    "                    \n",
    "                    if results:\n",
    "                        context_texts = [r[1] for r in results]\n",
    "                        print(\"\\nüìÑ Contexte trouv√© :\")\n",
    "                        for i, (_, corpus, distance) in enumerate(results):\n",
    "                            print(f\"  [{i+1}] {corpus[:70]}... (dist: {distance:.3f})\")\n",
    "                        \n",
    "                        print(\"\\nü§ñ G√©n√©ration de la r√©ponse...\\n\")\n",
    "                        answer = generate_answer_ollama(user_query, context_texts)\n",
    "                        print(f\"\\n{'='*60}\")\n",
    "                        print(f\"üí° R√âPONSE:\\n{answer}\")\n",
    "                        print(f\"{'='*60}\\n\")\n",
    "                    else:\n",
    "                        print(\"‚ùå Aucun contexte trouv√©.\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur : {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
